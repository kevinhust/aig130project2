# AWS Deployment Ready ‚úÖ
# AIG130 Project 2 - Bitcoin Price Prediction

**Status:** Ready for AWS Deployment
**Date:** 2025-11-20
**Mode:** AWS ECS + S3

---

## üéØ Deployment Configuration

### Current Settings

‚úÖ **Data Source:** AWS S3
‚úÖ **Container Platform:** AWS ECS Fargate
‚úÖ **Image Registry:** AWS ECR
‚úÖ **CI/CD:** GitHub Actions
‚úÖ **Monitoring:** CloudWatch Logs

---

## üìã Configuration Verification

### 1. Dockerfile (AWS Mode)
```dockerfile
# Data file NOT copied into image - loaded from S3
# COPY --chown=appuser:appuser data/btc_1h_data_2018_to_2025.csv ./data/  ‚ùå COMMENTED OUT
```
‚úÖ **Status:** Configured for S3 data loading

### 2. Environment Variables (ECS Task Definition)
```json
{
  "USE_S3": "true",
  "S3_BUCKET": "aig130-p2-ml-data-bucket",
  "S3_KEY": "data/btc_1h_data_2018_to_2025.csv",
  "AWS_DEFAULT_REGION": "us-east-1"
}
```
‚úÖ **Status:** Configured in task definition

### 3. Data Loader (src/data_loader.py)
```python
# Checks USE_S3 environment variable
if use_s3:
    load_from_s3(s3_bucket, s3_key, data_path)
else:
    load_from_local_file(data_path)
```
‚úÖ **Status:** S3 loading implemented with boto3

### 4. Dependencies (requirements.txt)
```
boto3==1.34.162
botocore==1.34.162
```
‚úÖ **Status:** AWS SDK included

### 5. GitHub Actions Workflow (.github/workflows/deploy.yml)
```yaml
- Upload training data to S3
- Build Docker image (without data)
- Push to ECR
- Deploy to ECS with USE_S3=true
```
‚úÖ **Status:** Complete CI/CD pipeline configured

---

## üîê Security Configuration

### IAM Roles Required

#### 1. ECS Task Execution Role
- **Name:** `aig130-p2-ecs-task-execution-role`
- **Purpose:** Pull images from ECR, write to CloudWatch
- **Policy:** `AmazonECSTaskExecutionRolePolicy`

#### 2. ECS Task Role
- **Name:** `aig130-p2-ecs-task-role`
- **Purpose:** Access S3 data bucket
- **Policy:** Custom S3 read access to `aig130-p2-ml-data-bucket`

#### 3. GitHub Actions User
- **Name:** `github-actions-aig130-p2`
- **Purpose:** Deploy from GitHub to AWS
- **Permissions:** ECR push, ECS deploy, S3 upload

### GitHub Secrets Required

| Secret Name | Value | Status |
|-------------|-------|--------|
| `AWS_ACCESS_KEY_ID` | GitHub Actions user access key | ‚è≥ TO BE SET |
| `AWS_SECRET_ACCESS_KEY` | GitHub Actions user secret key | ‚è≥ TO BE SET |

**‚ö†Ô∏è IMPORTANT:** Set these secrets before pushing to trigger deployment!

---

## üèóÔ∏è AWS Resources Checklist

### Required Resources

- [ ] **S3 Bucket:** `aig130-p2-ml-data-bucket`
  - Upload: `data/btc_1h_data_2018_to_2025.csv`
  - Enable versioning (optional)

- [ ] **ECR Repository:** `aig130-p2-ml-pipeline-ecr`
  - Enable image scanning
  - Lifecycle policy (optional)

- [ ] **ECS Cluster:** `aig130-p2-ml-cluster`
  - Fargate capacity provider

- [ ] **ECS Task Definition:** `aig130-p2-ml-task`
  - CPU: 1024 (1 vCPU)
  - Memory: 2048 (2 GB)
  - Environment variables: USE_S3=true, S3_BUCKET, S3_KEY

- [ ] **VPC & Security Group:** `aig130-p2-ml-sg`
  - Allow outbound to ECR, S3, CloudWatch

- [ ] **CloudWatch Log Group:** `/ecs/aig130-p2-ml-task`
  - Retention: 7 days

### Setup Commands

See `AWS_DEPLOYMENT_GUIDE.md` Section 1 for complete setup commands.

Quick setup:
```bash
# 1. Create IAM roles and policies
# 2. Create S3 bucket and upload data
aws s3 cp AIG130_Project2_Docker/data/btc_1h_data_2018_to_2025.csv \
  s3://aig130-p2-ml-data-bucket/data/btc_1h_data_2018_to_2025.csv

# 3. Create ECR repository
aws ecr create-repository --repository-name aig130-p2-ml-pipeline-ecr

# 4. Create ECS cluster
aws ecs create-cluster --cluster-name aig130-p2-ml-cluster

# 5. Register task definition (see guide for JSON)
aws ecs register-task-definition --cli-input-json file://task-definition.json

# 6. Create CloudWatch log group
aws logs create-log-group --log-group-name /ecs/aig130-p2-ml-task
```

---

## üöÄ Deployment Flow

### When you push to main:

```
1. GitHub Actions Triggered
   ‚Üì
2. AWS Credentials Configured (from Secrets)
   ‚Üì
3. Data Uploaded to S3 (if not exists)
   ‚Üì
4. Docker Image Built (WITHOUT data file)
   ‚Üì
5. Image Pushed to ECR
   ‚Üì
6. ECS Task Definition Updated
   ‚Üì
7. ECS Task Launched (Fargate)
   ‚Üì
8. Container Starts ‚Üí Loads data from S3
   ‚Üì
9. ML Pipeline Executes
   ‚Üì
10. Results Logged to CloudWatch
```

---

## üîç Verification Steps

### Before Deployment

```bash
# 1. Verify S3 bucket exists
aws s3 ls s3://aig130-p2-ml-data-bucket/data/

# 2. Verify ECR repository exists
aws ecr describe-repositories --repository-names aig130-p2-ml-pipeline-ecr

# 3. Verify ECS cluster exists
aws ecs describe-clusters --clusters aig130-p2-ml-cluster

# 4. Verify IAM roles exist
aws iam get-role --role-name aig130-p2-ecs-task-execution-role
aws iam get-role --role-name aig130-p2-ecs-task-role

# 5. Verify GitHub Secrets are set
gh secret list
# Should show: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
```

### After Deployment

```bash
# 1. Check GitHub Actions workflow
open https://github.com/kevinhust/aig130project2/actions

# 2. List running tasks
aws ecs list-tasks --cluster aig130-p2-ml-cluster

# 3. View logs
aws logs tail /ecs/aig130-p2-ml-task --follow

# 4. Check task status
aws ecs describe-tasks --cluster aig130-p2-ml-cluster --tasks <TASK_ARN>
```

---

## üìä Expected Behavior

### Container Startup

```
INFO:src.data_loader:USE_S3 environment variable detected - loading from S3
INFO:src.data_loader:Loading data from S3: s3://aig130-p2-ml-data-bucket/data/btc_1h_data_2018_to_2025.csv
INFO:src.data_loader:Downloaded data from S3 to /app/data/btc_1h_data_2018_to_2025.csv
INFO:src.data_loader:Loaded Bitcoin data from S3: 68831 rows, 11 columns
```

### Pipeline Execution

```
[1/6] Loading Bitcoin data... ‚úÖ
[2/6] Engineering features... ‚úÖ
[3/6] Splitting data... ‚úÖ
[4/6] Training models... ‚úÖ
[5/6] Evaluating models... ‚úÖ
[6/6] Generating visualizations... ‚úÖ

PIPELINE COMPLETED SUCCESSFULLY
```

---

## ‚ö†Ô∏è Important Notes

### 1. Data File NOT in Docker Image
- Docker image size: ~400 MB (down from 812 MB)
- Data loaded from S3 at runtime
- Reduces image build time and storage costs

### 2. Environment Variables Critical
The container **REQUIRES** these environment variables to load from S3:
```bash
USE_S3=true
S3_BUCKET=aig130-p2-ml-data-bucket
S3_KEY=data/btc_1h_data_2018_to_2025.csv
```

These are set in the ECS task definition.

### 3. IAM Permissions Required
- **Task Role** must have S3 read access to the data bucket
- Without proper permissions, S3 download will fail
- Task will fall back to synthetic data if S3 fails

### 4. Costs
- **S3 Storage:** ~$0.001/month (50 MB)
- **ECR Storage:** ~$0.05/month (500 MB)
- **ECS Fargate:** ~$0.09/run (10 minutes)
- **CloudWatch:** ~$0.25/month (logs)
- **Total per run:** ~$0.09
- **Monthly (1/day):** ~$2.80

---

## üéØ Deployment Checklist

### Pre-Deployment
- [x] Dockerfile configured for S3 (data COPY commented out)
- [x] S3 loading implemented in data_loader.py
- [x] boto3 added to requirements.txt
- [x] GitHub Actions workflow configured
- [x] Task definition includes USE_S3=true
- [ ] AWS resources created (see checklist above)
- [ ] GitHub Secrets configured

### First Deployment
1. Complete AWS setup (follow `AWS_DEPLOYMENT_GUIDE.md`)
2. Configure GitHub Secrets
3. Push to main branch
4. Monitor GitHub Actions workflow
5. Check CloudWatch logs for success

### Subsequent Deployments
1. Make code changes
2. Commit and push to main
3. GitHub Actions automatically deploys
4. Monitor CloudWatch logs

---

## üìö Documentation Reference

| Document | Purpose | When to Use |
|----------|---------|-------------|
| `README_AWS_DEPLOYMENT.md` | Entry point | Start here |
| `DEPLOYMENT_SUMMARY.md` | Overview | Understand architecture |
| `AWS_DEPLOYMENT_GUIDE.md` | Complete setup | First-time AWS setup |
| `AWS_DEPLOYMENT_QUICKSTART.md` | Quick reference | Daily operations |
| `AWS_DEPLOYMENT_READY.md` | This file | Pre-deployment check |
| `LOCAL_DOCKER_TEST_RESULTS.md` | Test validation | Verify local build |

---

## üîÑ Switching Between Local and AWS

### For Local Testing
```dockerfile
# In Dockerfile, uncomment:
COPY --chown=appuser:appuser data/btc_1h_data_2018_to_2025.csv ./data/
```
```bash
docker build -t ml-pipeline:local .
docker run --rm ml-pipeline:local
```

### For AWS Deployment
```dockerfile
# In Dockerfile, comment out:
# COPY --chown=appuser:appuser data/btc_1h_data_2018_to_2025.csv ./data/
```
```bash
git add .
git commit -m "Ready for AWS deployment"
git push origin main
```

---

## ‚úÖ Current Status Summary

| Component | Status | Notes |
|-----------|--------|-------|
| **Code** | ‚úÖ Ready | S3 loading implemented |
| **Dockerfile** | ‚úÖ Ready | Data COPY commented out |
| **Dependencies** | ‚úÖ Ready | boto3 included |
| **GitHub Actions** | ‚úÖ Ready | Workflow configured |
| **Task Definition** | ‚úÖ Ready | Environment vars set |
| **AWS Resources** | ‚è≥ Pending | Needs manual setup |
| **GitHub Secrets** | ‚è≥ Pending | Needs configuration |
| **S3 Data Upload** | ‚è≥ Pending | Needs manual upload |

---

## üöÄ Next Steps

1. **Set up AWS resources** (30 minutes)
   ```bash
   # Follow AWS_DEPLOYMENT_GUIDE.md Section 1
   ```

2. **Upload data to S3** (1 minute)
   ```bash
   aws s3 cp AIG130_Project2_Docker/data/btc_1h_data_2018_to_2025.csv \
     s3://aig130-p2-ml-data-bucket/data/btc_1h_data_2018_to_2025.csv
   ```

3. **Configure GitHub Secrets** (2 minutes)
   - Go to: Settings ‚Üí Secrets ‚Üí New secret
   - Add `AWS_ACCESS_KEY_ID`
   - Add `AWS_SECRET_ACCESS_KEY`

4. **Deploy!** (Automatic)
   ```bash
   # Already done! Just wait for AWS setup
   # Then push will trigger deployment
   ```

---

**Configuration Complete!** ‚úÖ
**Ready for AWS Deployment!** üöÄ‚òÅÔ∏è

When AWS resources are set up and GitHub Secrets are configured, the next push to main will automatically deploy to AWS ECS with S3 data loading.
